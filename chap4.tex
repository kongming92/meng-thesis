\chapter{Performance Optimization of the Split-Value System}

After verifying the correctness of the implementation, we turn to performance. The performance of an electronic voting system is an important consideration when selecting among various options. The system must scale to be able to handle hundreds of thousands or even millions of voters per district and perhaps multiple simultaneous races. Furthermore, the media and the public demand fast, accurate reporting of election results. We would like to be able to provide the election results along with a verified proof within hours of the closing of polling sites.

One proposed advantage of the split-value system is its efficiency. Existing comparable systems such as [STAR-VOTE] use relatively time-consuming public key encryption operations such as modular exponentiation, while the split-value system uses much simpler and faster modular addition operations. Compared to other systems, the split-value system pays a higher cost due to the need for $2m$ repetitions. Furthermore, the cost of inter-process and inter-machine communication and data serialization become non-negligible.

The implementation presented in [CITE MARCO], referred to as the \emph{reference implementation} here, contained numerous opportunities for optimization. In this section, we begin with a survey of the performance of cryptographic primitives and data serializers used heavily in the implementation. Then we present the main optimizations that were successfully applied to the reference implementation and their impact and conclude with an analysis of the scalability of the system.

All timings reported in this section, unless otherwise noted, are performed on a 16 core AWS machine using two Intel E5-2666 v3 processors operating at 2.9 GHz and 32 GB RAM. The default settings for simulating an election consist of a single race with two candidates and a third write-in candidate up to sixteen characters, 1000 voters, and $2m = 24$.

\section{Performance of Cryptographic Primitives}

\subsection{Hashing and Encryption Functions}

[TODO: TABLE] shows the performance of various cryptographic primitives, including hash functions (MD5, SHA-256), a block cipher (AES in CBC mode), and a stream cipher (XSalsa20), by running each function ten million times. The built-in Python hash functions and XSalsa20 perform efficiently as expected, and we choose SHA-256 as the hash function used in the implementation as it is both fast and secure.

It is surprising that the Python NaCl and Cryptography third-party libraries, both of which are very popular, significantly underperform compared to the built-in functions. The inefficiency of AES is particularly surprising given the 2010 release of the Intel AES New Instructions Set, which gives hardware support to all standard modes of operation and key lengths. The expanded instruction set gives very good performance; encryption with 256-bit AES-CBC takes 5.65 CPU cycles per byte (256-bit AES-CBC is the most expensive operation) [TODO CITE THIS]. The CPUs used in this benchmark did indeed support the expanded instruction set, but it is likely that the third-party libraries did not use them.

Unfortunately, the original design estimated the performance of AES to be more than two magnitudes faster [CITE THIS] than its Python implementation. Even using the significantly faster HMAC with SHA-256, commitments are more than forty times slower than estimated. It is likely that C code written using the expanded hardware instructions will offer significant performance benefits. For our Python implementation, we use HMAC with SHA-256 as the commitment function.

\subsection{Pseudorandom Number Generators}

As discussed in [LINK TO CH 3], randomness is used throughout the system. The method used in the implementation gives a good pseudorandom generator with three calls to the SHA-256 hash function (once to generate the new seed, and twice to compute the tweaked hash which becomes the output). However, as seen in [TODO: TABLE], there is a non-negligible overhead likely due to the repeated calls to the \texttt{digest} function and the byte-encoding functions used to incorporate the tweaking string.

When benchmarking the two functions side by side, it appears that \texttt{os.urandom(n)}, which provides ``a string of \texttt{n} random bytes suitable for cryptographic use'' [CITE PYTHON] is a faster option. However, in the context of the overall system, the improvement is minimal. Furthermore, \texttt{os.urandom} relies on the operating system's implementation of the \texttt{/dev/urandom} entropy pool, which is not always guaranteed to have enough entropy to produce good randomness. For these reasons, we stick to the method of repeatedly hashing the seed string to generate randomness.

\section{Performance of JSON Serializers}

The JSON format is used to serialize all data sent over the wire and posted to the secure bulletin board; it was chosen because it is compact, very commonly used, and well-supported by all platforms. The library that is used has a signficant impact on the performance of the system.

[TODO: TABLE] shows the performance of three common JSON serialization libraries for Python, tested on a moderately large (164.6 MB) JSON file found in [CITE GITHUB]. The \texttt{simplejson} and \texttt{ujson} libraries have C extensions that can be compiled for performance boosts; they were included in our tests. Our tests show that the \texttt{ujson} library significantly outperforms the other two libraries; the C extensions for \texttt{simplejson} do not provide any noticeable performance benefit.

Unfortunately we could not use \texttt{ujson} because it does not support integers longer than 64 bits. We started our implementation with Python's built-in JSON serializer; however, we discovered that the structure of the data in our particular system heavily favored \texttt{simplejson}. This was not reflected in the data used in the benchmarks in [TODO: TABLE]. Switching libraries produced significant speedups.

\section{Optimization of the Prototype}

To effectively optimize the reference implementation, we begin by collecting timing information on each phase of the simulation. We delve further into the performance of each function call by using the Python profiler, which tells us how much time is spent in each function, including and excluding the time spent in functions called from it. From there, we generally prioritize optimizing the most time-consuming functions as well examining the most frequently called functions to determine if any unnecessary work is being done.

We focus on the timings of the mix, proof, and verification phases as they are the most time-consuming. In a real election, the vote generation would occur throughout the day rather than all at once, so it is not a bottleneck in the system. The setup occurs before the election, and the tallying is very fast compared to the mix, proof, and verification phases.

[INSERT TABLE] shows the major optimizations that were performed on the reference implementations and the resulting runtimes. The performance gains for each optimization are reported relative to the row immediately preceding it in the table.

In summary, the most effective optimizations involved leveraging the available parallelism in the multiple mixservers and ensuring that commonly used subroutines were as fast as possible by optimizing by hand, using a built-in Python function, or selecting a third-party library optimized for speed. Overall, we observe a tenfold speedup due to our optimizations detailed in this section.

\subsection{The Python Profiler}

TODO: show sample profiler output

\subsection{Message Passing Over Sockets}

As explained in the previous chapter, communication between machines happens via TCP servers communicating over Python's standard socket implementation, which provides an interface for sending and receiving data.
\begin{itemize}
\item \texttt{socket.recv(bufsize)}. Receive data from the socket up to \texttt{bufsize} bytes, and return the data received.
\item \texttt{socket.send(string)}. Send data through the socket, and returns the number of bytes sent. \texttt{socket.send} does not guarantee that the all data is sent; the caller must check the return value and retry sending the remaining data if necessary.
\item \texttt{socket.sendall(string)}. Send data through the socket, and return \texttt{None} on success. Unlike \texttt{socket.send}, \texttt{socket.sendall} guarantees that all data is sent if the function returns successfully.
\end{itemize}
Due to constraints in the operating system, the value for \texttt{bufsize} used in the \texttt{socket.recv} function is usually 4096 or 8192, corresponding to the maximum length that may be sent via a single call to \texttt{socket.send}. Note that there is not a corresponding \texttt{socket.recvall} function. If a longer message is desired, we must repeatedly call \texttt{socket.recv} until the entire message is received. The reference implementation handled this situation by sleeping for a short amount of time after each call to \texttt{socket.recv} before attempting to receive more data from the socket.

Because \texttt{socket.recv} blocks if the data is not ready yet (up to some timeout), we do not need to sleep to wait for more data; rather, we just need to know how much data to expect and continuously call \texttt{socket.recv} until that length is reached. We define the following higher-level interface for communication over sockets:
\begin{itemize}
\item \texttt{send(socket, message)}. Prepend \texttt{message} with its length and send the resulting bytes over \texttt{socket}.
\item \texttt{recv(socket)}. Receive and return all data from the socket by stripping out the length from the first chunk of the message and repeatedly calling \texttt{socket.recv} until the length is met or an error is encountered. We assume that the length of a message will never exceed the limit of a 32-bit signed integer.
\end{itemize}

This optimization yielded a speedup of 20.9 seconds ($68.7\%$) in the proof phase and 43.5 seconds ($43.9\%$) in the verify phase.

\subsection{Byte/Integer Utility Functions}

Our profiler output revealed that a significant amount of time was spent on the reference implementation's utility functions for converting between Python bytes, integers, and hex strings. The conversions are necessary because different components of the system expect values to be in different forms; for example, the library used for cryptographic hash functions returns a byte array, while our secure bulletin board, to which the results of computing these cryptographic hashes are posted, accepts string inputs.

Specifically, for the \texttt{sv.bytes2hex} implementation, which returns the hex string representation of a byte array, we achieved a $15 \times$ speedup in the function's runtime by simply replacing the byte-by-byte conversion with the built-in \texttt{binascii.hexlify} function.

For the \texttt{sv.int2bytes} function, which converts any integer into a bytearray representation, we use Python 3's new \texttt{bit\_length} attribute, which supports signed integers up to 64 bits long, along with a 64-bit mask and shift. This allows us to process the input eight bytes at a time instead of one, yielding a $1.5 \times$ speedup in the function's runtime compared to the reference implementation.

Overall, these optimizations give a $6.1\%$ speedup, mostly in the proof and verify phases.

\subsection{Introducing Parallelism}

No performance analysis is complete without an analysis of the potential for parallelism. Multicore CPUs are now the norm; even a typical laptop that we would expect an election official to own and use in running the system is likely to have at least two CPU cores. Furthermore, in our system, the computation required to mix the votes and construct a proof of the election outcome is distributed across the nine mixsers. We show here that significant parallelism can be leveraged in each of the mix, proof, and verification phases of the system.

\subsubsection{Parallelism in the Mix and Proof Phases}

In the reference implementation of the mix and proof phases, the controller server issued remote procedure calls (RPCs) to each mixserver sequentially. Each RPC would block on the controller until the coresponding mixserver's RPC response arrived at the controller; only then would the next RPC be issued. Because the RPCs are independent within each part of the proof and verify phases, they may be issued asynchronously and in parallel.

We use Python's implementation of processes in the \texttt{multiprocessing} library to accomplish this parallelization. One process is forked off the process running the controller server for each RPC. The process is responsible for issuing the RPC, waiting for the response, and returning to the parent process. Even with a single-core controller server, a process that is waiting for an RPC response may yield the CPU to allow another process to issue an RPC.

Processes are used instead of threads because Python's Global Interpreter Lock limits parallelism between the various threads spawned from a single process. We confirmed that using Python threads instead of processes yielded no noticeable speedup. The built-in \texttt{multiprocessing} library does not support process pools as it does thread pools, so we must fork new processes for each RPC instead of relying on existing workers. However, forking a new process is a constant-time cost relative to scaling the number of voters and our tests show that it is negligible.

Introducing parallelism produced a 12.1 second ($194.8\%$) speedup in the mix phase and 21.5 second ($36.2\%$) speedup in the proof phase.

\subsubsection{Parallelism in the Verification Phase}

Unlike the mix and proof phases which involve many distinct machines, the verification phase occurs on a single machine. However, all parts of the verification process only depend on the contents of the secure bulletin board; after all, we expect that any member of the public may take the contents of the secure bulletin board and verify the election outcome herself. Although no part of the verification phase depends on any previous part because each part may independently read the secure bulletin board, in practice it is easier to let earlier phases populate a shared data structure for later phases to use. This prevents the need to deserialize the secure bulletin board, which may be a large file and thus a costly process, at the cost of some parallelism.

In the reference implementation, later parts of the verification phase used data that earlier parts had already verified. This makes verification simpler because at no point does the verifier need to worry about inconsistent or unverified data propagating through it. However, this scheme resulted in a complex data dependency graph which made the verifier difficult to parallelize; a process running one part of the verification could be started only when all of its dependencies had completed.

The requirement that data consumed in later parts must itself be verified is stricter than necessary. If any step of the verifier fails, we declare the election to be invalid, so even if bad data propagates through the verifier, at some point that data will itself be checked and an error produced. This check might not happen until after the bad data has been used in other parts of the verifier, but the problem will be caught nonetheless.

Relaxing our requirements on the verifier and introducing parallelism resulted in a 8 second ($8\%$) speedup in the verification phase.

\subsection{JSON Serialization}

While the benchmarks in [LINK TO SECTION ABOVE] showed no benefit in switching to the \texttt{simplejson} library, in our system the performance speedup was enormous. By simply switching the serializer library and eliminating indentation and formatting from JSON strings, a 41.5 second ($233\%$) speedup in the proof phase and 2.9 second ($40.5\%$). The overall runtime speedup for the simulation was 45 seconds ($1.26\%$).

\subsection{Hashing the Secure Bulletin Board}

Hashes of the secure bulletin board are used in generating the cut-and-choose and left-right challenges during the construction of the proof. They are also used in the verification to ensure that the hash value used in the proof are actually proper hashes of the secure bulletin board.

Because the verification process itself makes modifications to the secure bulletin board data structure, causing the hash values to differ, the reference implementation makes a deep copy of the data structure before proceeding with verification. By moving the hashing verification to be before the rest of the data structure is verified, a 1.3 second ($22\%$) speedup was obtained.

In the reference implementation, the mixservers ask the server storing the secure bulletin board for its hash value for the aforementioned reasons. Although the mixservers operate in parallel, the secure bulletin board runs on a single process and therefore serializes the requests for the hash value. Therefore, a mixserver that issues its request later must wait for the secure bulletin board to service all earlier requests, making no progress in the process. In our optimization, the controller server requests the hash value and sends it to each mixserver as part of the body of the relevant requests. Each mixserver then has all the information necessary to construct the entire proof section at the beginning of each phase and wastes no time waiting for the single-threaded secure bulletin board to service other requests. This optimization resulted in a 11.7 second ($208\%$) speedup.

Each request for the hash value of the secure bulletin board requires that the contents of the data structures held in memory be serialized before it can be hashed. As the secure bulletin board grows, this serialization becomes expensive. We can reduce this cost by computing the updated hash value every time anything is appended to the bulletin board. We calculate the new hash value by concatenating the data to be appended with the previous hash value. Because the previous hash value is always of fixed size, the cost to compute the hash value does not grow as the size of the data structures grows. While this results in a relatively small ($13.5\%$) performance gain in the proof phase, it prevents performance from diminishing as the size of the secure bulletin board increases.

\section{Performance Scalability}

In the previous section, we simulated elections with 1000 voters because it provided useful data for optimization without taking too long to run successive simulations. A real election, however, may have hundreds of thousands or even millions of voters. We are interested in how our system scales with the number of voters.

[TODO:TABLE] shows the performance of the system with 100, 500, 1000, 5000, and 10,000 voters and the corresponding costs per voter. We see that the performance of the system scales roughly linearly as the number of voters, which bodes well for scaling the system to production. In the current prototype and the testing machine, connections begin to time out for 100,000 voters. This is likely an issue with the pool of TCP connections becoming saturated since the simulation is running on a single operating system. However, this remains to be tested rigorously.

[TODO:TABLE] shows the performance of the system with the number of repetitions ($2m$) set to 4, 8, 16, and 24. Although the runtime grows very slightly sublinearly as the number of repetitions, reducing the number of repetitions may reduce the runtime significantly.

\section{Comparison with Other Systems}


\section{Conclusion}

In this section, we examined the performance of the split-value system implementation. Benchmarking functions available for cryptographic primitives led to the usage of the SHA-256 hash function and the HMAC commitment scheme in our implementation. We used the Python profiler to identify areas of the implementation that could yield large performance gains. By leveraging parallelism, reducing the wait time on sockets and requests to the secure bulletin board, and a more efficient JSON serializer, our optimized system runs in less than one-tenth the time of the reference implementation.

With the optimized implementation, an election of one million voters will take between seven and eight hours to mix, prove, tally, and verify. This is a bit more than twice as long as the time generally allotted between the closing of polling sites and the release of election results in the evening news. We hope that further optimization may yield this factor of two to three improvement in the runtime.

Using properly hardware-optimized AES encryption as a commitment scheme has the potential to reduce runtime significantly; according to Intel's measurements, the estimate of 8 million commitments given in [CITE PAPER] per second can be achieved even with conservative estimates.

In the current implementation, considerable time is spent on repeating the mixing and proof construction for $2m = 24$ times. As mentioned in [LINK TO PREV CHAPTER], this repetition is necessary due to the reduced security in using a Fiat-Shamir hash of the secure bulletin board instead of true randomness. The process may be sped up at least twofold if election officials adopted a true source of randomness during the construction of the proof.

Overall, it appears that under ideal situations (access to hardware functions, minimal cost in serialization and communication), the split-value system delivers on its promise to provide security without the heavy computation of most existing cryptographic solutions. However, its implementation requires careful optimization and selection of third-party code.
